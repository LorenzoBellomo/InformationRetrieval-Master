{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c583c9",
   "metadata": {},
   "source": [
    "# TEXTUAL ANNOTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293521f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For REST calls\n",
    "import json # for modelling objects in the JSON format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5fa839c",
   "metadata": {},
   "source": [
    "Now open the file config.json, which contains the key required for making REST requests to the SoBigData server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b041d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line opens the file from the file system, the file is in the same folder of the notebook and it is opened in \"read-only mode\"\n",
    "with open(\"../config.json\", 'r') as json_file:\n",
    "    config = json.load(json_file) # load the json object inside the config file\n",
    "    KEY = config['d4science_KEY'] # this is the key we will be using for REST calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87ec817",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGME_ENDPOINT = \"https://tagme.d4science.org/tagme/tag\"\n",
    "LANG = \"en\" # Also works in italian and german"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8cc017c",
   "metadata": {},
   "source": [
    "Now create the function that will \"wrap\" the REST call. It needs a textual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225be2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tagme(text):\n",
    "    payload = {\"text\": text, \"gcube-token\": KEY, \"lang\": LANG}\n",
    "    # Now we issue a post HTTP request\n",
    "    r = requests.post(TAGME_ENDPOINT, payload)\n",
    "    if r.status_code != 200:\n",
    "        # this means something went wrong with the query\n",
    "        raise Exception(\"Error on text: {}\\n{}\".format(text, r.text))\n",
    "    return r.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d255a5b",
   "metadata": {},
   "source": [
    "And now we display the result for a simple textual query. The interesting part, for us, is under the key _annotations_.\n",
    "This will be a list of annotations containing the following fields:\n",
    "- **spot (string)**: how the anchor appears in the text.\n",
    "- **start (int)**: the index of the starting character of the anchor.\n",
    "- **end (int)**: the index of the ending character of the anchor.\n",
    "- **link_probability (float ‚àà[ùüé,ùüè])**: number of times that the spot is an anchor in Wikipedia / number of occurrences of the spot in Wikipedia.\n",
    "- **rho (float ‚àà[ùüé,ùüè])**: semantic coherency of the entity with respect to the query.\n",
    "- **id (int)**: the Wikipedia identifier of the page _(https://en.wikipedia.org/?curid=<>)_.\n",
    "- **title (string)**: title of the Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c867c7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': '5',\n",
       " 'annotations': [{'spot': 'Italy',\n",
       "   'start': 0,\n",
       "   'link_probability': 0.4437723457813263,\n",
       "   'rho': 0.4525856375694275,\n",
       "   'end': 5,\n",
       "   'id': 362466,\n",
       "   'title': 'Italy national football team'},\n",
       "  {'spot': 'will',\n",
       "   'start': 6,\n",
       "   'link_probability': 0.0036389119923114777,\n",
       "   'rho': 0.06729841977357864,\n",
       "   'end': 10,\n",
       "   'id': 32828260,\n",
       "   'title': 'Will (2011 film)'},\n",
       "  {'spot': '2022 world cup',\n",
       "   'start': 35,\n",
       "   'link_probability': 0.3492063581943512,\n",
       "   'rho': 0.3398236632347107,\n",
       "   'end': 49,\n",
       "   'id': 17742072,\n",
       "   'title': '2022 FIFA World Cup'}],\n",
       " 'time': 35,\n",
       " 'api': 'tag',\n",
       " 'lang': 'en',\n",
       " 'timestamp': '2023-01-02T08:56:34'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_text = \"Italy will not be competing in the 2022 world cup\"\n",
    "resp = query_tagme(short_text)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c21bd",
   "metadata": {},
   "source": [
    "## Handle longer texts / filtering noisy annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3298ffde",
   "metadata": {},
   "source": [
    "TagME has been designed for handling short texts, but we also have a way to obtain competitive results on longer ones. \n",
    "This requires modifying the window of spots that are checked by TagME when doing disambiguation.\n",
    "\n",
    "Now open a new text file with a slightly longer text and annotate it with TagME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39f92ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leonardo da Vinci was an Italian Renaissance polymath whose areas of interest included invention, painting, sculpting, architecture, science, music, mathematics, engineering, literature, anatomy, geology, astronomy, botany, writing, history, and cartography. \\nHe has been variously called the father of palaeontology, ichnology, and architecture, and is widely considered one of the greatest painters of all time. Leonardo is revered for his technological ingenuity. He conceptualised flying machines, a type of armoured fighting vehicle, concentrated solar power, an adding machine, and the double hull.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/Leonardo.txt\", 'r') as long_file:\n",
    "    # the text is not a json object, it is just a plaintext, so just read it regularly with read()\n",
    "    text = long_file.read()\n",
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f71d3357",
   "metadata": {},
   "source": [
    "Now we will change the tagging function we made before, by adding an optional boolean parameter. If true, this means that the text is long, otherwise it is short and we can avoid changing the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1b9f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tagme(text, long_text=False):\n",
    "    payload = {\"text\": text, \"gcube-token\": KEY, \"lang\": LANG}\n",
    "    if long_text:\n",
    "        # long_text is by defaul false, but if specified by the user, we set the window size at 5\n",
    "        payload[\"long_text\"] = 5\n",
    "    r = requests.post(TAGME_ENDPOINT, payload)\n",
    "    if r.status_code != 200:\n",
    "        raise Exception(\"Error on text: {}\\n{}\".format(text, r.text))\n",
    "    return r.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00e7dc7f",
   "metadata": {},
   "source": [
    "But how do we deal with noisy annotations? TagME gives us a \"content relevance\" score in the form of the **Rho-score**.\n",
    "We can filter the lowest ranked annotations on relevancy to remove noise. A common threshold for this task is 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e8f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing the min_rho parameter and see how it impacts the returned entities\n",
    "def get_tagme_entities(tagme_response, min_rho=0.3):\n",
    "    ann = tagme_response[\"annotations\"]\n",
    "    ann = [a for a in ann if a[\"rho\"] > min_rho] # filter all the annotations with a rho score lower than the threshold\n",
    "    return [a[\"title\"] for a in ann if \"title\" in a] # return just the page titles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1799d1a",
   "metadata": {},
   "source": [
    "Now see which entities _disappear_ when filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02b543b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE FILTERING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Leonardo da Vinci',\n",
       " 'Leonardo da Vinci',\n",
       " 'Italian Renaissance',\n",
       " 'Polymath',\n",
       " 'Attention',\n",
       " 'Invention',\n",
       " 'Painting',\n",
       " 'Sculpture',\n",
       " 'Architecture',\n",
       " 'Science',\n",
       " 'Music and mathematics',\n",
       " 'Engineering',\n",
       " 'Literature',\n",
       " 'Anatomy',\n",
       " 'Geology',\n",
       " 'Astronomy',\n",
       " 'Botany',\n",
       " 'Writing',\n",
       " 'History',\n",
       " 'Cartography',\n",
       " 'Clergy',\n",
       " 'Paleontology',\n",
       " 'Ichnology',\n",
       " 'Architecture',\n",
       " 'Neoplatonism',\n",
       " 'Greatest!',\n",
       " 'Painting',\n",
       " 'Time (magazine)',\n",
       " 'Leonardo da Vinci',\n",
       " 'Canonization',\n",
       " 'Technology',\n",
       " 'Ingenuity',\n",
       " 'Concept',\n",
       " 'Flying Machines s.r.o.',\n",
       " 'Granite',\n",
       " 'Stellar classification',\n",
       " 'Armoured fighting vehicle',\n",
       " 'Concentrated solar power',\n",
       " 'Adding machine',\n",
       " 'Double hull']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"BEFORE FILTERING\")\n",
    "resp = query_tagme(text, long_text=True) \n",
    "before_filtering = [a[\"title\"] for a in resp['annotations'] if \"title\" in a]\n",
    "before_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6fe3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER FILTERING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Leonardo da Vinci',\n",
       " 'Leonardo da Vinci',\n",
       " 'Italian Renaissance',\n",
       " 'Polymath',\n",
       " 'Music and mathematics',\n",
       " 'Geology',\n",
       " 'Astronomy',\n",
       " 'Botany',\n",
       " 'Cartography',\n",
       " 'Paleontology',\n",
       " 'Ichnology',\n",
       " 'Armoured fighting vehicle',\n",
       " 'Concentrated solar power',\n",
       " 'Adding machine']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"AFTER FILTERING\")\n",
    "after_filtering = get_tagme_entities(resp)\n",
    "after_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f04019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The annotations that were filtered out are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Attention',\n",
       " 'Invention',\n",
       " 'Painting',\n",
       " 'Sculpture',\n",
       " 'Architecture',\n",
       " 'Science',\n",
       " 'Engineering',\n",
       " 'Literature',\n",
       " 'Anatomy',\n",
       " 'Writing',\n",
       " 'History',\n",
       " 'Clergy',\n",
       " 'Architecture',\n",
       " 'Neoplatonism',\n",
       " 'Greatest!',\n",
       " 'Painting',\n",
       " 'Time (magazine)',\n",
       " 'Canonization',\n",
       " 'Technology',\n",
       " 'Ingenuity',\n",
       " 'Concept',\n",
       " 'Flying Machines s.r.o.',\n",
       " 'Granite',\n",
       " 'Stellar classification',\n",
       " 'Double hull']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The annotations that were filtered out are:\")\n",
    "[a for a in before_filtering if a not in after_filtering]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff61e1",
   "metadata": {},
   "source": [
    "# TRY OTHER ANNOTATORS: SWAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "948ba75a",
   "metadata": {},
   "source": [
    "TagME is not the only available annotator. There are several more, each one with its own strenghts and weaknesses.\n",
    "Most of the available annotators are available at [this](https://sobigdata.d4science.org/web/tagme/service-overview) page on the SoBigData Infrastructure\n",
    "\n",
    "**SWAT** is specifically a salient entity linker, which works best on long, well-constructed texts.\n",
    "The fields returned are:\n",
    "- **salience_class (int)**: 1 if the entity is deemed salient, 0 otherwise\n",
    "- **salience_score (float ‚àà[ùüé,ùüè])**: the saliency of the enitity in the text (similar to the rho-score in tagme)\n",
    "- **spans (list)**: list of times where this entity appears, they are described as:\n",
    "    - *start (int)*: the index of the starting character of the anchor\n",
    "    - *end (int)*: the index of the ending character of the anchor\n",
    "- **wiki_id (int)**: the Wikipedia identifier of the page\n",
    "- **wiki_title (string)**: title of the Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08382a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'salience_class': 1.0,\n",
       "  'salience_score': 0.9471508264541626,\n",
       "  'spans': [{'end': 17, 'start': 0}, {'end': 422, 'start': 414}],\n",
       "  'wiki_id': 18079,\n",
       "  'wiki_title': 'Leonardo_da_Vinci'},\n",
       " {'salience_class': 1.0,\n",
       "  'salience_score': 0.5190669894218445,\n",
       "  'spans': [{'end': 32, 'start': 25}],\n",
       "  'wiki_id': 14532,\n",
       "  'wiki_title': 'Italy'},\n",
       " {'salience_class': 1.0,\n",
       "  'salience_score': 0.5682003498077393,\n",
       "  'spans': [{'end': 44, 'start': 33}],\n",
       "  'wiki_id': 25532,\n",
       "  'wiki_title': 'Renaissance'},\n",
       " {'salience_class': 0.0,\n",
       "  'salience_score': 0.4803982079029083,\n",
       "  'spans': [{'end': 65, 'start': 60}],\n",
       "  'wiki_id': 9630,\n",
       "  'wiki_title': 'Ecology'},\n",
       " {'salience_class': 0.0,\n",
       "  'salience_score': 0.35197311639785767,\n",
       "  'spans': [{'end': 77, 'start': 69}],\n",
       "  'wiki_id': 146738,\n",
       "  'wiki_title': 'Interest'},\n",
       " {'salience_class': 0.0,\n",
       "  'salience_score': 0.42167073488235474,\n",
       "  'spans': [{'end': 96, 'start': 87}],\n",
       "  'wiki_id': 44312,\n",
       "  'wiki_title': 'Invention'},\n",
       " {'salience_class': 1.0,\n",
       "  'salience_score': 0.6101479530334473,\n",
       "  'spans': [{'end': 106, 'start': 98}, {'end': 400, 'start': 392}],\n",
       "  'wiki_id': 18622193,\n",
       "  'wiki_title': 'Painting'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the new URL of the annotator on the SoBigData Infrastructure\n",
    "SWAT_ENDPOINT = \"https://swat.d4science.org/salience\"\n",
    "\n",
    "# SWAT also requires a title of the content\n",
    "def query_swat(title, content):\n",
    "    document = json.dumps({\"title\": title, \"content\": content})\n",
    "    r = requests.post(SWAT_ENDPOINT, data = document, params={'gcube-token': KEY})\n",
    "    if r.status_code != 200:\n",
    "        raise Exception(\"Error on article titled: {}\\n{}\".format(title, r.text))\n",
    "    return r.json()[\"annotations\"]\n",
    "\n",
    "query_swat(\"Leonardo da Vinci\", text)[:7]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "564a5a82",
   "metadata": {},
   "source": [
    "# RELATEDNESS\n",
    "Ok but now that I have entities, how do I deal with them? I need to know which are similar and which are not.\n",
    "If we don't see any way of \"dealing with the entities\", how do we unlock its full potential? How is this method more powerful than dealing with generic words as tokens?\n",
    "\n",
    "There are several ways in which we can obtain the relatedness of couples of entities.\n",
    "The main one that is shown in this notebook is by querying TagME itself. TagME has an internal relatedness computation framework, so I can ask TagME itself how close two entities are to one another. This metric is computed directly on the Wikipedia Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82829fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL where the relatedness is given\n",
    "ENDPOINT_RELATEDNESS = \"https://tagme.d4science.org/tagme/rel\"\n",
    "\n",
    "# In case I need efficiency I can do batch queries of 100 couples per HTTP call\n",
    "def query_relatedness(e1, e2):\n",
    "    # Entities require underscores in-place of the spaces. The space is between entity one and entity two\n",
    "    tt = e1.replace(\" \", \"_\") + \" \" + e2.replace(\" \", \"_\")\n",
    "    payload = {\"tt\": tt, \"gcube-token\": KEY, \"lang\": LANG}\n",
    "    r = requests.post(ENDPOINT_RELATEDNESS, payload)\n",
    "    if r.status_code != 200:\n",
    "        raise Exception(\"Error on relatedness computation: {}\\n{}\".format(tt, r.text))\n",
    "    return r.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34c65cb4",
   "metadata": {},
   "source": [
    "Now let's test the relatedness of three entities. \n",
    "Two are closely related to one-another (biology and biotechnology).\n",
    "The last one is completely out of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e252769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'couple': 'Biology Biotechnology', 'rel': 0.6070536971092224}]\n",
      "[{'couple': 'Barack_Obama Biotechnology', 'rel': 0.23863035440444946}]\n",
      "[{'couple': 'Barack_Obama Biology', 'rel': 0.16491788625717163}]\n"
     ]
    }
   ],
   "source": [
    "first = query_relatedness(\"Biology\", \"Biotechnology\")\n",
    "second = query_relatedness(\"Barack Obama\", \"Biotechnology\")\n",
    "thirds = query_relatedness(\"Barack Obama\", \"Biology\")\n",
    "print(first['result'])\n",
    "print(second['result'])\n",
    "print(thirds['result'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59063527",
   "metadata": {},
   "source": [
    "# WIKIPEDIA2VEC\n",
    "This next section requires some additional setup and loading a large model file. I will just show you how it works but there is no need to execute it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb9eab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "MODEL_FILE = r\"D:\\wiki\\enwiki_20180420_100d.pkl\"\n",
    "\n",
    "wiki2vec = Wikipedia2Vec.load(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6c76002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_vector(e):\n",
    "    try:\n",
    "        emb = wiki2vec.get_entity_vector(e)\n",
    "    except:\n",
    "        raise Exception(\"Entity vector {} not found\\n\".format(e))\n",
    "    return emb\n",
    "\n",
    "def similarity(v1, v2):\n",
    "    x = np.array(v1).reshape(1,-1)\n",
    "    y = np.array(v2).reshape(1,-1)\n",
    "    return cosine_similarity(x, y)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "943b286a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Barack Obama',\n",
       " memmap([-1.7208770e-01,  6.8366393e-02, -2.2114021e-01,  1.8625689e-01,\n",
       "         -1.6881050e-01, -1.0273114e+00, -1.7403726e-01,  6.5132761e-01,\n",
       "         -1.1579229e+00, -1.1234688e-02, -6.3258129e-01, -3.6312324e-01,\n",
       "          1.8983840e+00, -1.0301901e+00, -6.0025400e-01, -7.8052205e-01,\n",
       "          2.5215951e-01, -2.7664509e-01, -5.1084882e-01,  1.2842940e-01,\n",
       "          6.3571817e-01,  8.1280574e-02, -1.2716837e+00,  4.2106557e-01,\n",
       "         -2.0525175e-01,  9.8252594e-02,  1.3547261e-01, -5.7749820e-01,\n",
       "          2.9801649e-01,  1.4131395e+00, -7.3676556e-01, -1.0151949e+00,\n",
       "         -1.1703007e-01,  1.2873930e+00, -2.9190008e-02, -2.9518047e-01,\n",
       "          1.4377959e-01,  3.0795303e-01,  1.4484618e+00,  1.7310138e-01,\n",
       "          1.4899757e-03, -8.5064566e-01, -1.7244501e-01, -7.6880109e-01,\n",
       "         -9.8339975e-01,  3.2403290e-01, -6.4912087e-01,  3.7392426e-01,\n",
       "         -3.1706643e-01,  3.8999528e-01,  7.0731849e-03,  4.8704663e-01,\n",
       "          8.8876688e-01, -8.4579253e-01,  4.3871808e-01,  4.0680537e-01,\n",
       "         -1.6622459e+00, -1.3958409e-01, -7.4907917e-01, -4.6015662e-01,\n",
       "          1.0252564e+00,  6.1859900e-01, -1.0264760e+00,  1.1253888e-02,\n",
       "          1.4748510e+00, -1.0670326e+00, -1.4506657e+00,  9.2815667e-02,\n",
       "          6.0668534e-01,  5.6702320e-02,  5.8442976e-02,  2.4479965e-02,\n",
       "          3.2270652e-01, -4.6916559e-01, -6.3365918e-01, -5.8209294e-01,\n",
       "          9.4899368e-01, -2.4084625e-01, -1.0808723e+00, -5.6064343e-01,\n",
       "          6.3331157e-01, -9.5565569e-01,  2.8229371e-01, -2.6353934e-01,\n",
       "          2.9214889e-02, -7.8398116e-02,  2.2097924e-01,  3.7298155e-01,\n",
       "          1.0195505e-01, -2.8144988e-01,  6.0816568e-01,  4.0925273e-01,\n",
       "          2.2093943e-01, -3.7608457e-01,  1.7633964e-01,  8.1662679e-01,\n",
       "          9.8994994e-01,  6.9826591e-01,  1.3987473e+00,  8.2485056e-01],\n",
       "        dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = (\"Barack Obama\", get_entity_vector(\"Barack Obama\"))\n",
    "v2 = (\"Biology\", get_entity_vector(\"Biology\"))\n",
    "v3 = (\"Biotechnology\", get_entity_vector(\"Biotechnology\"))\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b6dfbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Cosine similarity between Barack Obama and Biology is 0.19\n",
      "Cosine similarity between Barack Obama and Biotechnology is 0.16\n",
      "Cosine similarity between Biology and Biotechnology is 0.52\n"
     ]
    }
   ],
   "source": [
    "print(\"======================================================================\")\n",
    "from itertools import combinations\n",
    "for x, y in combinations([v1, v2, v3], 2):\n",
    "    print(\"Cosine similarity between {} and {} is {:.2f}\".format(x[0], y[0], similarity(x[1], y[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('IR')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c226b66ad212a3c8eabd6b6be3829f40d39277fa7bdc3bf63a77768ea80548ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
